{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BCI_WheelChair_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Kh3ET2VJkd5",
        "colab_type": "code",
        "outputId": "cada6d8c-0638-4fe2-f76e-1960b7490d09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        }
      },
      "source": [
        "!git clone https://github.com/NTX-McGill/NeuroTechX-McGill-2019.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'NeuroTechX-McGill-2019'...\n",
            "remote: Enumerating objects: 60, done.\u001b[K\n",
            "remote: Counting objects: 100% (60/60), done.\u001b[K\n",
            "remote: Compressing objects: 100% (47/47), done.\u001b[K\n",
            "remote: Total 4086 (delta 21), reused 38 (delta 13), pack-reused 4026\u001b[K\n",
            "Receiving objects: 100% (4086/4086), 326.19 MiB | 13.20 MiB/s, done.\n",
            "Resolving deltas: 100% (2184/2184), done.\n",
            "Checking out files: 100% (478/478), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9UxQo7IJUMI",
        "colab_type": "code",
        "outputId": "16f6bce7-5953-4545-e12f-bd60daef6514",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7hv3zYHKXGL",
        "colab_type": "code",
        "outputId": "8ae9a110-3882-4ff2-8c4a-bb78f7d801e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/drive/My\\ Drive/NeuroTechX-McGill-2019"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/NeuroTechX-McGill-2019\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLH5hFRRK9aa",
        "colab_type": "code",
        "outputId": "c10a7268-9f3b-4281-b648-20872fec6c5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd offline/ML"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/NeuroTechX-McGill-2019/offline/ML\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iL8esXBRI8sV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#from metadata import MARKER_DATA, DATA_COLUMNS, LABELS, ALL_FILES\n",
        "\n",
        "\n",
        "def merge_dols(dol1, dol2):\n",
        "    keys = set(dol1).union(dol2)\n",
        "    no = []\n",
        "    return dict((k, dol1.get(k, no) + dol2.get(k, no)) for k in keys)\n",
        "\n",
        "def merge_all_dols(arr):\n",
        "    all_data = {'Right': [], 'Left': [], 'Rest': []}\n",
        "    for dol in arr:\n",
        "        all_data = merge_dols(all_data, dol)\n",
        "    return all_data\n",
        "\n",
        "def load_openbci_raw(path):\n",
        "    data = np.loadtxt(path,\n",
        "                      delimiter=',',\n",
        "                      skiprows=7,\n",
        "                      usecols=DATA_COLUMNS)\n",
        "    eeg = data[:, :-1]\n",
        "    timestamps = data[:, -1]\n",
        "    return eeg, timestamps\n",
        "\n",
        "\n",
        "def load_data(csv):\n",
        "    print(\"loading \" + csv)\n",
        "    data = {label: [] for label in LABELS}\n",
        "    df = pd.read_csv('../' + csv)\n",
        "    path_arr = csv.split('/')\n",
        "    folder, fname = path_arr[:-1], path_arr[-1]\n",
        "    data_path = \"/\".join(['..'] + folder + [MARKER_DATA[fname]])\n",
        "    eeg, timestamps = load_openbci_raw(data_path)\n",
        "    prev = 0\n",
        "    prev_direction = df['Direction'][prev]\n",
        "    for idx, el in enumerate(df['Direction']):\n",
        "        if el != prev_direction or idx == len(df.index) - 1:\n",
        "            start = df['Time'][prev]\n",
        "            end = df['Time'][idx]\n",
        "            indices = np.where(\n",
        "                np.logical_and(\n",
        "                    timestamps >= start,\n",
        "                    timestamps <= end))\n",
        "            trial = eeg[indices]\n",
        "            data[prev_direction].append(trial)\n",
        "            prev = idx\n",
        "            prev_direction = el\n",
        "    return data\n",
        "\n",
        "\n",
        "def load_dataset(csv_set):\n",
        "    dataset = {label: [] for label in LABELS}\n",
        "    for csv in csv_set:\n",
        "        try:\n",
        "            data = load_data(csv)\n",
        "            dataset = merge_dols(dataset, data)\n",
        "        except Exception as e: print(e)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def load_all():\n",
        "    return {fname: load_data(fname) for fname in ALL_FILES}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_2BlumsIpGQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import numpy.fft as fft\n",
        "from scipy import signal\n",
        "import matplotlib.mlab as mlab\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#from metadata import SAMPLING_FREQ\n",
        "\n",
        "\n",
        "def filter_signal(arr, lowcut, highcut, order, notch=True):\n",
        "    if notch:\n",
        "        arr = notch_mains_interference(arr)\n",
        "    nyq = 0.5 * SAMPLING_FREQ\n",
        "    b, a = signal.butter(1, [lowcut / nyq, highcut / nyq], btype='band')\n",
        "    for i in range(0, order):\n",
        "        arr = signal.lfilter(b, a, arr, axis=0)\n",
        "    return arr\n",
        "\n",
        "\n",
        "def notch_mains_interference(data):\n",
        "    notch_freq_Hz = np.array([60.0])  # main + harmonic frequencies\n",
        "    for freq_Hz in np.nditer(notch_freq_Hz):  # loop over each target freq\n",
        "        bp_stop_Hz = freq_Hz + 3.0 * np.array([-1, 1])  # set the stop band\n",
        "        b, a = signal.butter(3, bp_stop_Hz / (SAMPLING_FREQ / 2.0), 'bandstop')\n",
        "        arr = signal.lfilter(b, a, data, axis=0)\n",
        "        print(\"Notch filter removing: \" +\n",
        "              str(bp_stop_Hz[0]) +\n",
        "              \"-\" +\n",
        "              str(bp_stop_Hz[1]) +\n",
        "              \" Hz\")\n",
        "    return arr\n",
        "\n",
        "\n",
        "def get_artifact_indices(ch):\n",
        "    start_indices = [0]\n",
        "    i = 0\n",
        "    while i < len(ch):\n",
        "        if ch[i] > 100:\n",
        "            start_indices.append(i)\n",
        "            i += 500\n",
        "        i += 1\n",
        "    return start_indices\n",
        "\n",
        "\n",
        "def get_psd(ch, fs_Hz, shift=0.1):\n",
        "    NFFT = fs_Hz * 2\n",
        "    overlap = NFFT - int(shift * fs_Hz)\n",
        "    psd, freqs = mlab.psd(np.squeeze(ch),\n",
        "                          NFFT=NFFT,\n",
        "                          window=mlab.window_hanning,\n",
        "                          Fs=fs_Hz,\n",
        "                          noverlap=overlap\n",
        "                          )  # returns PSD power per Hz\n",
        "    # convert the units of the spectral data\n",
        "    spec_PSDperBin = spec_PSDperHz * fs_Hz / float(NFFT)\n",
        "    return psd, freqs  # dB re: 1 uV\n",
        "\n",
        "\n",
        "def get_spectral_content(ch, fs_Hz, shift=0.1):\n",
        "    NFFT = fs_Hz * 2\n",
        "    overlap = NFFT - int(shift * fs_Hz)\n",
        "    spec_PSDperHz, spec_freqs, spec_t = mlab.specgram(np.squeeze(ch),\n",
        "                                                      NFFT=NFFT,\n",
        "                                                      window=mlab.window_hanning,\n",
        "                                                      Fs=fs_Hz,\n",
        "                                                      noverlap=overlap\n",
        "                                                      )  # returns PSD power per Hz\n",
        "    # convert the units of the spectral data\n",
        "    spec_PSDperBin = spec_PSDperHz * fs_Hz / float(NFFT)\n",
        "    return spec_t, spec_freqs, spec_PSDperBin  # dB re: 1 uV\n",
        "\n",
        "\n",
        "def plot_specgram(spec_freqs, spec_PSDperBin, title, shift, i=1):\n",
        "    f_lim_Hz = [0, 20]   # frequency limits for plotting\n",
        "    # plt.figure(figsize=(10,5))\n",
        "    spec_t = [idx * .1 for idx in range(len(spec_PSDperBin[0]))]\n",
        "    plt.subplot(3, 1, i)\n",
        "    plt.title(title)\n",
        "    plt.pcolor(spec_t, spec_freqs, 10 *\n",
        "               np.log10(spec_PSDperBin))  # dB re: 1 uV\n",
        "    plt.clim([-25, 26])\n",
        "    plt.xlim(spec_t[0], spec_t[-1] + 1)\n",
        "    plt.ylim(f_lim_Hz)\n",
        "    plt.xlabel('Time (sec)')\n",
        "    plt.ylabel('Frequency (Hz)')\n",
        "    plt.subplots_adjust(hspace=1)\n",
        "\n",
        "\n",
        "def resize_min(specgram, i=1):\n",
        "    min_length = min([len(el[0]) for el in specgram])\n",
        "    specgram = np.array([el[:, :min_length] for el in specgram])\n",
        "    return specgram\n",
        "\n",
        "\n",
        "def resize_max(specgram, fillval=np.nan):\n",
        "    max_length = max([len(el[0]) for el in specgram])\n",
        "    return np.array([pad_block(el, max_length, fillval) for el in specgram])\n",
        "\n",
        "\n",
        "def pad_block(block, max_length, fillval):\n",
        "    padding = np.full([len(block), max_length - (len(block[0]))], fillval)\n",
        "    return np.hstack((block, padding))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKntjk9QIgFU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ELECTRODE_C3 = 0\n",
        "ELECTRODE_C1 = 1\n",
        "ELECTRODE_C2 = -2\n",
        "ELECTRODE_C4 = -1\n",
        "\n",
        "SAMPLING_FREQ = 250\n",
        "\n",
        "DATA_COLUMNS = (1, 2, 3, 4, 5, 6, 7, 8, 13)\n",
        "LABELS = ['Left', 'Right', 'Rest']\n",
        "\n",
        "MARKER_DATA = {\"1_011_Rest20LeftRight20_MI-2019-3-24-16-25-41.csv\": '011_1to3_OpenBCI-RAW-2019-03-24_16-21-59.txt',\n",
        "               \"2_011_Rest20LeftRight20_MI-2019-3-24-16-38-10.csv\": '011_1to3_OpenBCI-RAW-2019-03-24_16-21-59.txt',\n",
        "               \"3_011_Rest20LeftRight10_MI-2019-3-24-16-49-23.csv\": '011_1to3_OpenBCI-RAW-2019-03-24_16-21-59.txt',\n",
        "               \"4_011_Rest20LeftRight10_MI-2019-3-24-16-57-8.csv\": '011_4to6_OpenBCI-RAW-2019-03-24_16-54-15.txt',\n",
        "               \"5_011_Rest20LeftRight20_MI-2019-3-24-17-3-17.csv\": '011_4to6_OpenBCI-RAW-2019-03-24_16-54-15.txt',\n",
        "          \n",
        "               }\n",
        "          \n",
        "FILES_BY_SUBJECT = [         \n",
        "             [\"data/March24_011/1_011_Rest20LeftRight20_MI-2019-3-24-16-25-41.csv\",  # 011\n",
        "              \"data/March24_011/2_011_Rest20LeftRight20_MI-2019-3-24-16-38-10.csv\",\n",
        "              \"data/March24_011/3_011_Rest20LeftRight10_MI-2019-3-24-16-49-23.csv\",\n",
        "              \"data/March24_011/4_011_Rest20LeftRight10_MI-2019-3-24-16-57-8.csv\",\n",
        "              \"data/March24_011/5_011_Rest20LeftRight20_MI-2019-3-24-17-3-17.csv\",\n",
        "              ]           \n",
        "             ]\n",
        "ALL_FILES = [name for sublist in FILES_BY_SUBJECT for name in sublist]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zgo_QzT3TTIh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxSuKbT0H1Bw",
        "colab_type": "code",
        "outputId": "b3611fb9-51d2-4169-f3a6-2d5813d5b3ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        }
      },
      "source": [
        "import warnings\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn import model_selection\n",
        "from sklearn.preprocessing import scale\n",
        "from sklearn.decomposition import PCA\n",
        "import seaborn as sn\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.mlab as mlab\n",
        "import numpy.fft as fft\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "from scipy import signal\n",
        "\n",
        "import sys\n",
        "sys.path.append('../utils')\n",
        "#from metadata import MARKER_DATA, LABELS, FILES_BY_SUBJECT, ALL_FILES, ELECTRODE_C3, ELECTRODE_C4\n",
        "#import file_utils\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, classes,\n",
        "                          normalize=False,\n",
        "                          title=None,\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if not title:\n",
        "        if normalize:\n",
        "            title = 'Normalized confusion matrix'\n",
        "        else:\n",
        "            title = 'Confusion matrix, without normalization'\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    # Only use the labels that appear in the data\n",
        "    classes = classes[unique_labels(y_true, y_pred)]\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    # We want to show all ticks...\n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\n",
        "           yticks=np.arange(cm.shape[0]),\n",
        "           # ... and label them with the respective list entries\n",
        "           xticklabels=classes, yticklabels=classes,\n",
        "           title=title,\n",
        "           ylabel='True label',\n",
        "           xlabel='Predicted label')\n",
        "\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "             rotation_mode=\"anchor\")\n",
        "\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], fmt),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    fig.tight_layout()\n",
        "    return ax\n",
        "\n",
        "\n",
        "\n",
        "def plot_specgram(spec_freqs, spec_PSDperBin, title, shift, i=1):\n",
        "    f_lim_Hz = [0, 20]   # frequency limits for plotting\n",
        "    spec_t = [idx*.1 for idx in range(len(spec_PSDperBin[0]))]\n",
        "    plt.subplot(3, 1, i)\n",
        "    plt.title(title)\n",
        "    plt.pcolor(spec_t, spec_freqs, 10*np.log10(spec_PSDperBin))  # dB re: 1 uV\n",
        "    plt.clim([-25, 26])\n",
        "    plt.xlim(spec_t[0], spec_t[-1]+1)\n",
        "    plt.ylim(f_lim_Hz)\n",
        "    plt.xlabel('Time (sec)')\n",
        "    plt.ylabel('Frequency (Hz)')\n",
        "    plt.subplots_adjust(hspace=1)\n",
        "\n",
        "\n",
        "def epoch_data(data, window_length, shift, maxlen=2560):\n",
        "    arr = []\n",
        "    start = 0\n",
        "    i = start\n",
        "    maxlen = min(len(data), maxlen)\n",
        "    while i + window_length < start + maxlen:\n",
        "        arr.append(data[i:i+window_length])\n",
        "        i += shift\n",
        "    return np.array(arr)\n",
        "\n",
        "\n",
        "def extract_features(all_data, window_s, shift, plot_psd=False, separate_trials=False, scale_by=None):\n",
        "    all_psds = {label: [] for label in LABELS}\n",
        "    all_features = {label: [] for label in LABELS}\n",
        "\n",
        "    idx = 1\n",
        "    for direction, data in all_data.items():\n",
        "        for trial in data:\n",
        "            epochs = epoch_data(trial, int(250 * window_s), int(shift*250))    # shape n x 500 x 2\n",
        "            trial_features = []\n",
        "            for epoch in epochs:\n",
        "                print('epoch_shape: ',epoch.shape)\n",
        "                features, freqs, psds_per_channel = get_features(epoch.T, scale_by=scale_by)\n",
        "                print('freqs:',freqs)\n",
        "                psd_c3, psd_c4 = psds_per_channel[0] , psds_per_channel[-1]\n",
        "                all_psds[direction].append([psd_c3, psd_c4])\n",
        "                trial_features.append(features)\n",
        "                \n",
        "                # Sanity check: plot the psd\n",
        "                if plot_psd:\n",
        "                    plt.figure(\"psd\")\n",
        "                    plt.subplot(3, 2, idx)\n",
        "                    plt.plot(freqs, psd_c3)\n",
        "                    plt.ylim([0, 25])\n",
        "                    plt.xlim([6, 20])\n",
        "                    plt.subplot(3, 2, idx+1)\n",
        "                    plt.plot(freqs, psd_c4)\n",
        "                    plt.ylim([0, 25])\n",
        "                    plt.xlim([6, 20])\n",
        "            if trial_features:\n",
        "                if separate_trials:\n",
        "                    all_features[direction].append(np.array(trial_features))\n",
        "                else:\n",
        "                    all_features[direction].extend(trial_features)\n",
        "        idx += 2\n",
        "    return all_psds, all_features, freqs\n",
        "\n",
        "\n",
        "def to_feature_vec(all_features, rest=False):\n",
        "    feature_arr = []\n",
        "    for direction, features in all_features.items():\n",
        "        features = np.array(features)\n",
        "        arr = np.hstack((features, np.full([features.shape[0], 1], LABELS.index(direction))))\n",
        "        feature_arr.append(arr)\n",
        "    if not rest or not len(features):\n",
        "        feature_arr = feature_arr[:-1]\n",
        "    return np.vstack(feature_arr)\n",
        "\n",
        "\n",
        "def normalize(features_dict):\n",
        "    \"\"\" Divide features by mean per channel \"\"\"\n",
        "    all_features = to_feature_vec(features_dict, rest=True)\n",
        "    av = list(np.mean(all_features, axis=0))\n",
        "    mean_coeff = np.array([el/sum(av[:-1]) for el in av[:-1]])\n",
        "    for direction, features in features_dict.items():\n",
        "        features = [np.divide(example, mean_coeff) for example in features]\n",
        "        features_dict[direction] = features\n",
        "    \n",
        "def running_mean(x, N):\n",
        "   cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
        "   return (cumsum[N:] - cumsum[:-N]) / N\n",
        "\n",
        "\n",
        "def evaluate_models(X, Y, X_test, Y_test, models):\n",
        "    \"\"\" Evaluate test accuracy of all models in a list of models\n",
        "    \n",
        "    Args:\n",
        "        X : array of features (train)\n",
        "        Y : array of labels (train)\n",
        "        X_test : array of features (test)\n",
        "        Y_test : array of labels (test)\n",
        "        models : list of (name, model) tuples\n",
        "    \n",
        "    Returns:\n",
        "        val_results : array of accuracies, shape num_models\n",
        "    \"\"\"\n",
        "    test_results = []\n",
        "    for name, model in models:\n",
        "        model.fit(X, Y)\n",
        "        score = model.score(X_test, Y_test)\n",
        "        test_results.append(score)\n",
        "        msg = \"%s: %f\" % (name, score)\n",
        "        print(msg)\n",
        "    return np.array(test_results)\n",
        "\n",
        "\n",
        "def evaluate_models_crossval(X, Y, models, scoring, random_state, n_splits=10):\n",
        "    \"\"\" Evaluate cross-validation accuracy of all models in a list of models\n",
        "    \n",
        "    Args:\n",
        "        X : array of features\n",
        "        Y : array of labels\n",
        "        models : list of (name, model) tuples\n",
        "        scoring : string, scoring metric\n",
        "        random_state : seed to use for shuffling\n",
        "        n_splits : number of folds\n",
        "    \n",
        "    Returns:\n",
        "        val_results : array of accuracies, shape num_models x n_splits\n",
        "    \"\"\"\n",
        "    val_results = []\n",
        "    for name, model in models:\n",
        "        kfold = model_selection.KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
        "        cv_results = model_selection.cross_val_score(model, X_test, Y_test, cv=kfold, scoring=scoring)\n",
        "        val_results.append(cv_results)\n",
        "        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "        print(msg)\n",
        "    return np.array(val_results)\n",
        "\n",
        "\n",
        "def get_features(arr, channels=[ELECTRODE_C3, ELECTRODE_C4], scale_by=None):\n",
        "    \"\"\" Get features from single window of EEG data\n",
        "    \n",
        "    Args:\n",
        "        arr : data of shape num_channels x timepoints\n",
        "    \n",
        "    Returns:\n",
        "        features : array with feature values\n",
        "        freqs : array of frequencies in Hz\n",
        "        psds_per_channel : array with full psd spectrum, shape num_channels x num_freqs\n",
        "    \"\"\"\n",
        "\n",
        "    psds_per_channel = []\n",
        "    nfft = 500\n",
        "    if arr.shape[-1] < 500:\n",
        "        nfft = 250\n",
        "    print(arr[channels])    \n",
        "    for ch in arr[channels]:        \n",
        "        #freqs,psd = signal.periodogram(np.squeeze(ch),fs=250, nfft=500, detrend='constant')\n",
        "        psd, freqs, = mlab.psd(np.squeeze(ch),NFFT=nfft,Fs=250)\n",
        "        print('freqs: ',freqs)\n",
        "        psds_per_channel.append(psd)\n",
        "    psds_per_channel = np.array(psds_per_channel)\n",
        "    mu_indices = np.where(np.logical_and(freqs >= 10, freqs <= 12))\n",
        "    \n",
        "    #features = np.amax(psds_per_channel[:,mu_indices], axis=-1).flatten()   # max of 10-12hz as feature\n",
        "    features = np.mean(psds_per_channel[:, mu_indices], axis=-1).flatten()   # mean of 10-12hz as feature\n",
        "    if scale_by:\n",
        "        scale_indices = np.where(np.logical_and(freqs >= scale_by[0], freqs <= scale_by[-1]))\n",
        "        scales = np.mean(psds_per_channel[:,scale_indices],axis=-1).flatten()\n",
        "        temp.append(scales)\n",
        "        features = np.divide(features, scales)\n",
        "    #features = np.array([features[:2].mean(), features[2:].mean()])\n",
        "    # features = psds_per_channel[:,mu_indices].flatten()                     # all of 10-12hz as feature\n",
        "    return features, freqs, psds_per_channel\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    shift = 0.25\n",
        "    plot_psd = False\n",
        "    tmin, tmax = 0, 0\n",
        "    window_lengths = [1, 2, 4]  # window lengths in seconds\n",
        "    normalize_spectra = True\n",
        "    run_pca = False\n",
        "    scale_by = None\n",
        "    \n",
        "    # Load data\n",
        "    dataset = load_all()\n",
        "    subjects = [i for i in range(len(FILES_BY_SUBJECT))]             # index of the test files we want to use\n",
        "    \n",
        "    all_val_results = []\n",
        "    all_test_results = []\n",
        "\n",
        "    # Test options and evaluation metric\n",
        "    scoring = 'accuracy'\n",
        "    validation = True\n",
        "    test = False\n",
        "    seed = 7\n",
        "    models = []\n",
        "    models.append(('LR', LogisticRegression(solver='lbfgs')))\n",
        "    models.append(('LDA', LinearDiscriminantAnalysis()))\n",
        "    models.append(('KNN', KNeighborsClassifier()))\n",
        "    models.append(('CART', DecisionTreeClassifier()))\n",
        "    models.append(('NB', GaussianNB(var_smoothing=0.001)))\n",
        "    models.append(('SVM', SVC(gamma='scale')))\n",
        "    \n",
        "    # Perform leave-one-subject-out cross-validation for each subject\n",
        "    # Delivers accuracy by subject, then by window size, then by session, then by model\n",
        "    for subj in subjects:\n",
        "        test_csvs = FILES_BY_SUBJECT[subj]\n",
        "        train_csvs = [el for el in ALL_FILES if el not in test_csvs]\n",
        "        train_data = merge_all_dols([dataset[csv] for csv in train_csvs])\n",
        "        \n",
        "        # Print subject name\n",
        "        print(test_csvs[0].split('/')[1])\n",
        "        print('Puta prueba')\n",
        "        window_val_results = []\n",
        "        window_test_results = []\n",
        "        print(type(train_data))\n",
        "        print(window_s)\n",
        "        \n",
        "        for window_s in window_lengths:\n",
        "            train_psds, train_features, freqs = extract_features(train_data, window_s, shift, plot_psd, scale_by=scale_by)\n",
        "            data = to_feature_vec(train_features, rest=False)\n",
        "            \n",
        "            # X, Y for training\n",
        "            # For testing: X_test, Y_test\n",
        "            X = data[:, :-1]\n",
        "            Y = data[:, -1]\n",
        "            X, Y = shuffle(X, Y, random_state=seed)\n",
        "            if run_pca:\n",
        "                pca = PCA(n_components=2, svd_solver='full')\n",
        "                pca.fit(X)\n",
        "                X = pca.transform(X)\n",
        "            \n",
        "            subj_val_results = []\n",
        "            subj_test_results = []\n",
        "            for csv in test_csvs:\n",
        "                test_data = dataset[csv]\n",
        "                _, test_features, _ = extract_features(test_data, window_s, shift, plot_psd, scale_by=scale_by)\n",
        "                if normalize_spectra:\n",
        "                    normalize(test_features)\n",
        "                test_data = to_feature_vec(test_features)\n",
        "                X_test = test_data[:, :-1]\n",
        "                Y_test = test_data[:, -1]\n",
        "                if run_pca:\n",
        "                    X_test = pca.transform(X_test)\n",
        "                \n",
        "                if validation:\n",
        "                    print(\"VALIDATION\")\n",
        "                    val_results = evaluate_models_crossval(X_test, Y_test, models, scoring, seed, n_splits=10)\n",
        "                    print(\"average accuracy: \" + \"{:2.1f}\".format(val_results.mean() * 100))\n",
        "                    subj_val_results.append(val_results.mean() * 100)\n",
        "                    \n",
        "                if test:\n",
        "                    print(\"TEST\")\n",
        "                    test_results = evaluate_models(X, Y, X_test, Y_test, models)\n",
        "                    print(\"average accuracy: {:2.1f}\".format(test_results.mean() * 100))\n",
        "                    subj_test_results.append(test_results.mean() * 100)\n",
        "            window_val_results.append([np.array(subj_val_results).mean(), stats.sem(np.array(subj_val_results))])\n",
        "            window_test_results.append([np.array(subj_test_results).mean(),stats.sem(np.array(subj_test_results))])\n",
        "        all_val_results.append(window_val_results)\n",
        "        all_test_results.append(window_test_results)\n",
        "        \n",
        "    print(np.array(all_test_results).mean())\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading data/March24_011/1_011_Rest20LeftRight20_MI-2019-3-24-16-25-41.csv\n",
            "loading data/March24_011/2_011_Rest20LeftRight20_MI-2019-3-24-16-38-10.csv\n",
            "loading data/March24_011/3_011_Rest20LeftRight10_MI-2019-3-24-16-49-23.csv\n",
            "loading data/March24_011/4_011_Rest20LeftRight10_MI-2019-3-24-16-57-8.csv\n",
            "loading data/March24_011/5_011_Rest20LeftRight20_MI-2019-3-24-17-3-17.csv\n",
            "March24_011\n",
            "Puta prueba\n",
            "<class 'dict'>\n",
            "1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-dbd68cff324d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mwindow_s\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwindow_lengths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m             \u001b[0mtrain_psds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreqs_5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshift\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_psd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_by\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscale_by\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_feature_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-47-dbd68cff324d>\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(all_data, window_s, shift, plot_psd, separate_trials, scale_by)\u001b[0m\n\u001b[1;32m    142\u001b[0m                     \u001b[0mall_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0midx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mall_psds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreqs_3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'freqs_3' referenced before assignment"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vr9aujLH49j",
        "colab_type": "code",
        "outputId": "629f70d3-97f6-4d87-c371-8b84bd387e78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_data\n",
        "\n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Left': [], 'Rest': [], 'Right': []}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fz3RNCSygzfY",
        "colab_type": "code",
        "outputId": "07243e86-4ee9-42d2-ed4d-3911f36826cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "MARKER_DATA"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'10_008-2019-3-22-15-8-55.csv': '10_008_OpenBCI-RAW-2019-03-22_15-07-58.txt',\n",
              " '4_RestLeftRight_MI_5s.csv': '4_RestLeftRight_5s_MI_OpenBCI-RAW-2019-03-17_16-32-53.txt',\n",
              " '5_RestLeftRight_MI_10s.csv': '5_RestLeftRight_10s_OpenBCI-RAW-2019-03-17_16-37-32.txt'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qa57okSGiG3F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "matplotlib.pyplot.psd(x, NFFT=None, Fs=None, Fc=None, detrend=None, window=None, noverlap=None, pad_to=None, sides=None, scale_by_freq=None, return_line=None, *, data=None, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g05xn4gK5Knv",
        "colab_type": "code",
        "outputId": "bcd7c321-f5c1-43e1-d442-831cfccbc4a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "psd, freqs = mlab.psd(np.squeeze([8,3]),NFFT=500,Fs=250)\n",
        "print(freqs)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  0.    0.5   1.    1.5   2.    2.5   3.    3.5   4.    4.5   5.    5.5\n",
            "   6.    6.5   7.    7.5   8.    8.5   9.    9.5  10.   10.5  11.   11.5\n",
            "  12.   12.5  13.   13.5  14.   14.5  15.   15.5  16.   16.5  17.   17.5\n",
            "  18.   18.5  19.   19.5  20.   20.5  21.   21.5  22.   22.5  23.   23.5\n",
            "  24.   24.5  25.   25.5  26.   26.5  27.   27.5  28.   28.5  29.   29.5\n",
            "  30.   30.5  31.   31.5  32.   32.5  33.   33.5  34.   34.5  35.   35.5\n",
            "  36.   36.5  37.   37.5  38.   38.5  39.   39.5  40.   40.5  41.   41.5\n",
            "  42.   42.5  43.   43.5  44.   44.5  45.   45.5  46.   46.5  47.   47.5\n",
            "  48.   48.5  49.   49.5  50.   50.5  51.   51.5  52.   52.5  53.   53.5\n",
            "  54.   54.5  55.   55.5  56.   56.5  57.   57.5  58.   58.5  59.   59.5\n",
            "  60.   60.5  61.   61.5  62.   62.5  63.   63.5  64.   64.5  65.   65.5\n",
            "  66.   66.5  67.   67.5  68.   68.5  69.   69.5  70.   70.5  71.   71.5\n",
            "  72.   72.5  73.   73.5  74.   74.5  75.   75.5  76.   76.5  77.   77.5\n",
            "  78.   78.5  79.   79.5  80.   80.5  81.   81.5  82.   82.5  83.   83.5\n",
            "  84.   84.5  85.   85.5  86.   86.5  87.   87.5  88.   88.5  89.   89.5\n",
            "  90.   90.5  91.   91.5  92.   92.5  93.   93.5  94.   94.5  95.   95.5\n",
            "  96.   96.5  97.   97.5  98.   98.5  99.   99.5 100.  100.5 101.  101.5\n",
            " 102.  102.5 103.  103.5 104.  104.5 105.  105.5 106.  106.5 107.  107.5\n",
            " 108.  108.5 109.  109.5 110.  110.5 111.  111.5 112.  112.5 113.  113.5\n",
            " 114.  114.5 115.  115.5 116.  116.5 117.  117.5 118.  118.5 119.  119.5\n",
            " 120.  120.5 121.  121.5 122.  122.5 123.  123.5 124.  124.5 125. ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crgr5L7O5LwV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "058b05ee-78bf-4364-c5a8-efe2fdbbd83c"
      },
      "source": [
        "freqs,psd = signal.periodogram(np.squeeze([0.787,0.453,0.876]),fs=250, nfft=500, detrend='constant')\n",
        "print(freqs, psd)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  0.    0.5   1.    1.5   2.    2.5   3.    3.5   4.    4.5   5.    5.5\n",
            "   6.    6.5   7.    7.5   8.    8.5   9.    9.5  10.   10.5  11.   11.5\n",
            "  12.   12.5  13.   13.5  14.   14.5  15.   15.5  16.   16.5  17.   17.5\n",
            "  18.   18.5  19.   19.5  20.   20.5  21.   21.5  22.   22.5  23.   23.5\n",
            "  24.   24.5  25.   25.5  26.   26.5  27.   27.5  28.   28.5  29.   29.5\n",
            "  30.   30.5  31.   31.5  32.   32.5  33.   33.5  34.   34.5  35.   35.5\n",
            "  36.   36.5  37.   37.5  38.   38.5  39.   39.5  40.   40.5  41.   41.5\n",
            "  42.   42.5  43.   43.5  44.   44.5  45.   45.5  46.   46.5  47.   47.5\n",
            "  48.   48.5  49.   49.5  50.   50.5  51.   51.5  52.   52.5  53.   53.5\n",
            "  54.   54.5  55.   55.5  56.   56.5  57.   57.5  58.   58.5  59.   59.5\n",
            "  60.   60.5  61.   61.5  62.   62.5  63.   63.5  64.   64.5  65.   65.5\n",
            "  66.   66.5  67.   67.5  68.   68.5  69.   69.5  70.   70.5  71.   71.5\n",
            "  72.   72.5  73.   73.5  74.   74.5  75.   75.5  76.   76.5  77.   77.5\n",
            "  78.   78.5  79.   79.5  80.   80.5  81.   81.5  82.   82.5  83.   83.5\n",
            "  84.   84.5  85.   85.5  86.   86.5  87.   87.5  88.   88.5  89.   89.5\n",
            "  90.   90.5  91.   91.5  92.   92.5  93.   93.5  94.   94.5  95.   95.5\n",
            "  96.   96.5  97.   97.5  98.   98.5  99.   99.5 100.  100.5 101.  101.5\n",
            " 102.  102.5 103.  103.5 104.  104.5 105.  105.5 106.  106.5 107.  107.5\n",
            " 108.  108.5 109.  109.5 110.  110.5 111.  111.5 112.  112.5 113.  113.5\n",
            " 114.  114.5 115.  115.5 116.  116.5 117.  117.5 118.  118.5 119.  119.5\n",
            " 120.  120.5 121.  121.5 122.  122.5 123.  123.5 124.  124.5 125. ] [4.10865055e-36 3.33644074e-09 1.33563568e-08 3.00915209e-08\n",
            " 5.35948587e-08 8.39404052e-08 1.21223244e-07 1.65559430e-07\n",
            " 2.17085891e-07 2.75960320e-07 3.42361042e-07 4.16486868e-07\n",
            " 4.98556932e-07 5.88810510e-07 6.87506825e-07 7.94924828e-07\n",
            " 9.11362975e-07 1.03713897e-06 1.17258953e-06 1.31807006e-06\n",
            " 1.47395440e-06 1.64063449e-06 1.81852007e-06 2.00803832e-06\n",
            " 2.20963351e-06 2.42376662e-06 2.65091498e-06 2.89157186e-06\n",
            " 3.14624604e-06 3.41546141e-06 3.69975654e-06 3.99968418e-06\n",
            " 4.31581083e-06 4.64871628e-06 4.99899307e-06 5.36724604e-06\n",
            " 5.75409177e-06 6.16015811e-06 6.58608359e-06 7.03251693e-06\n",
            " 7.50011643e-06 7.98954944e-06 8.50149181e-06 9.03662723e-06\n",
            " 9.59564671e-06 1.01792480e-05 1.07881348e-05 1.14230165e-05\n",
            " 1.20846071e-05 1.27736251e-05 1.34907924e-05 1.42368339e-05\n",
            " 1.50124768e-05 1.58184500e-05 1.66554835e-05 1.75243075e-05\n",
            " 1.84256519e-05 1.93602457e-05 2.03288163e-05 2.13320887e-05\n",
            " 2.23707851e-05 2.34456238e-05 2.45573190e-05 2.57065798e-05\n",
            " 2.68941098e-05 2.81206061e-05 2.93867588e-05 3.06932506e-05\n",
            " 3.20407557e-05 3.34299391e-05 3.48614566e-05 3.63359533e-05\n",
            " 3.78540636e-05 3.94164102e-05 4.10236036e-05 4.26762413e-05\n",
            " 4.43749074e-05 4.61201718e-05 4.79125897e-05 4.97527009e-05\n",
            " 5.16410291e-05 5.35780817e-05 5.55643487e-05 5.76003023e-05\n",
            " 5.96863968e-05 6.18230671e-05 6.40107289e-05 6.62497781e-05\n",
            " 6.85405898e-05 7.08835183e-05 7.32788963e-05 7.57270344e-05\n",
            " 7.82282207e-05 8.07827205e-05 8.33907756e-05 8.60526036e-05\n",
            " 8.87683982e-05 9.15383283e-05 9.43625373e-05 9.72411435e-05\n",
            " 1.00174239e-04 1.03161890e-04 1.06204136e-04 1.09300989e-04\n",
            " 1.12452434e-04 1.15658429e-04 1.18918904e-04 1.22233760e-04\n",
            " 1.25602871e-04 1.29026081e-04 1.32503208e-04 1.36034036e-04\n",
            " 1.39618325e-04 1.43255803e-04 1.46946169e-04 1.50689093e-04\n",
            " 1.54484214e-04 1.58331142e-04 1.62229458e-04 1.66178711e-04\n",
            " 1.70178422e-04 1.74228082e-04 1.78327150e-04 1.82475058e-04\n",
            " 1.86671205e-04 1.90914963e-04 1.95205672e-04 1.99542644e-04\n",
            " 2.03925161e-04 2.08352474e-04 2.12823808e-04 2.17338356e-04\n",
            " 2.21895283e-04 2.26493726e-04 2.31132793e-04 2.35811563e-04\n",
            " 2.40529089e-04 2.45284394e-04 2.50076476e-04 2.54904304e-04\n",
            " 2.59766822e-04 2.64662946e-04 2.69591567e-04 2.74551550e-04\n",
            " 2.79541735e-04 2.84560937e-04 2.89607947e-04 2.94681532e-04\n",
            " 2.99780436e-04 3.04903379e-04 3.10049059e-04 3.15216154e-04\n",
            " 3.20403316e-04 3.25609182e-04 3.30832363e-04 3.36071456e-04\n",
            " 3.41325034e-04 3.46591655e-04 3.51869856e-04 3.57158160e-04\n",
            " 3.62455071e-04 3.67759077e-04 3.73068653e-04 3.78382258e-04\n",
            " 3.83698335e-04 3.89015317e-04 3.94331624e-04 3.99645662e-04\n",
            " 4.04955828e-04 4.10260507e-04 4.15558077e-04 4.20846904e-04\n",
            " 4.26125348e-04 4.31391762e-04 4.36644490e-04 4.41881872e-04\n",
            " 4.47102244e-04 4.52303935e-04 4.57485274e-04 4.62644584e-04\n",
            " 4.67780188e-04 4.72890409e-04 4.77973568e-04 4.83027988e-04\n",
            " 4.88051992e-04 4.93043906e-04 4.98002060e-04 5.02924787e-04\n",
            " 5.07810425e-04 5.12657317e-04 5.17463814e-04 5.22228272e-04\n",
            " 5.26949056e-04 5.31624541e-04 5.36253111e-04 5.40833159e-04\n",
            " 5.45363091e-04 5.49841324e-04 5.54266289e-04 5.58636430e-04\n",
            " 5.62950205e-04 5.67206089e-04 5.71402570e-04 5.75538157e-04\n",
            " 5.79611371e-04 5.83620757e-04 5.87564876e-04 5.91442308e-04\n",
            " 5.95251655e-04 5.98991541e-04 6.02660610e-04 6.06257530e-04\n",
            " 6.09780992e-04 6.13229710e-04 6.16602424e-04 6.19897898e-04\n",
            " 6.23114924e-04 6.26252318e-04 6.29308925e-04 6.32283617e-04\n",
            " 6.35175295e-04 6.37982888e-04 6.40705355e-04 6.43341686e-04\n",
            " 6.45890899e-04 6.48352046e-04 6.50724208e-04 6.53006501e-04\n",
            " 6.55198070e-04 6.57298096e-04 6.59305791e-04 6.61220402e-04\n",
            " 6.63041212e-04 6.64767534e-04 6.66398721e-04 6.67934157e-04\n",
            " 6.69373264e-04 6.70715499e-04 6.71960357e-04 6.73107366e-04\n",
            " 6.74156094e-04 6.75106144e-04 6.75957157e-04 6.76708811e-04\n",
            " 6.77360821e-04 6.77912940e-04 6.78364960e-04 6.78716708e-04\n",
            " 6.78968053e-04 6.79118897e-04 3.39584593e-04]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjgZaP9X6lEX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}