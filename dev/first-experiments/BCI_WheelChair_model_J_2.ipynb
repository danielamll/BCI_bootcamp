{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BCI_WheelChair_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Kh3ET2VJkd5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/NTX-McGill/NeuroTechX-McGill-2019.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9UxQo7IJUMI",
        "colab_type": "code",
        "outputId": "7c3038d4-0b3a-46ce-f53d-9becb17058a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7hv3zYHKXGL",
        "colab_type": "code",
        "outputId": "1f4db965-94f5-4fea-fba5-53f43419cebb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/drive/My\\ Drive/NeuroTechX-McGill-2019"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/NeuroTechX-McGill-2019\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLH5hFRRK9aa",
        "colab_type": "code",
        "outputId": "f540f14b-93f8-487a-c11e-892fe1c77d3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd offline/ML"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/NeuroTechX-McGill-2019/offline/ML\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iL8esXBRI8sV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#from metadata import MARKER_DATA, DATA_COLUMNS, LABELS, ALL_FILES\n",
        "\n",
        "\n",
        "def merge_dols(dol1, dol2):\n",
        "    keys = set(dol1).union(dol2)\n",
        "    no = []\n",
        "    return dict((k, dol1.get(k, no) + dol2.get(k, no)) for k in keys)\n",
        "\n",
        "def merge_all_dols(arr):\n",
        "    all_data = {'Right': [], 'Left': [], 'Rest': []}\n",
        "    for dol in arr:\n",
        "        all_data = merge_dols(all_data, dol)\n",
        "    return all_data\n",
        "\n",
        "def load_openbci_raw(path):\n",
        "    data = np.loadtxt(path,\n",
        "                      delimiter=',',\n",
        "                      skiprows=7,\n",
        "                      usecols=DATA_COLUMNS)\n",
        "    eeg = data[:, :-1]\n",
        "    timestamps = data[:, -1]\n",
        "    return eeg, timestamps\n",
        "\n",
        "\n",
        "def load_data(csv):\n",
        "    print(\"loading \" + csv)\n",
        "    data = {label: [] for label in LABELS}\n",
        "    df = pd.read_csv('../' + csv)\n",
        "    path_arr = csv.split('/')\n",
        "    folder, fname = path_arr[:-1], path_arr[-1]\n",
        "    data_path = \"/\".join(['..'] + folder + [MARKER_DATA[fname]])\n",
        "    eeg, timestamps = load_openbci_raw(data_path)\n",
        "    prev = 0\n",
        "    prev_direction = df['Direction'][prev]\n",
        "    for idx, el in enumerate(df['Direction']):\n",
        "        if el != prev_direction or idx == len(df.index) - 1:\n",
        "            start = df['Time'][prev]\n",
        "            end = df['Time'][idx]\n",
        "            indices = np.where(\n",
        "                np.logical_and(\n",
        "                    timestamps >= start,\n",
        "                    timestamps <= end))\n",
        "            trial = eeg[indices]\n",
        "            data[prev_direction].append(trial)\n",
        "            prev = idx\n",
        "            prev_direction = el\n",
        "    return data\n",
        "\n",
        "\n",
        "def load_dataset(csv_set):\n",
        "    dataset = {label: [] for label in LABELS}\n",
        "    for csv in csv_set:\n",
        "        try:\n",
        "            data = load_data(csv)\n",
        "            dataset = merge_dols(dataset, data)\n",
        "        except Exception as e: print(e)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def load_all():\n",
        "    return {fname: load_data(fname) for fname in ALL_FILES}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_2BlumsIpGQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import numpy.fft as fft\n",
        "from scipy import signal\n",
        "import matplotlib.mlab as mlab\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#from metadata import SAMPLING_FREQ\n",
        "\n",
        "\n",
        "def filter_signal(arr, lowcut, highcut, order, notch=True):\n",
        "    if notch:\n",
        "        arr = notch_mains_interference(arr)\n",
        "    nyq = 0.5 * SAMPLING_FREQ\n",
        "    b, a = signal.butter(1, [lowcut / nyq, highcut / nyq], btype='band')\n",
        "    for i in range(0, order):\n",
        "        arr = signal.lfilter(b, a, arr, axis=0)\n",
        "    return arr\n",
        "\n",
        "\n",
        "def notch_mains_interference(data):\n",
        "    notch_freq_Hz = np.array([60.0])  # main + harmonic frequencies\n",
        "    for freq_Hz in np.nditer(notch_freq_Hz):  # loop over each target freq\n",
        "        bp_stop_Hz = freq_Hz + 3.0 * np.array([-1, 1])  # set the stop band\n",
        "        b, a = signal.butter(3, bp_stop_Hz / (SAMPLING_FREQ / 2.0), 'bandstop')\n",
        "        arr = signal.lfilter(b, a, data, axis=0)\n",
        "        print(\"Notch filter removing: \" +\n",
        "              str(bp_stop_Hz[0]) +\n",
        "              \"-\" +\n",
        "              str(bp_stop_Hz[1]) +\n",
        "              \" Hz\")\n",
        "    return arr\n",
        "\n",
        "\n",
        "def get_artifact_indices(ch):\n",
        "    start_indices = [0]\n",
        "    i = 0\n",
        "    while i < len(ch):\n",
        "        if ch[i] > 100:\n",
        "            start_indices.append(i)\n",
        "            i += 500\n",
        "        i += 1\n",
        "    return start_indices\n",
        "\n",
        "\n",
        "def get_psd(ch, fs_Hz, shift=0.1):\n",
        "    NFFT = fs_Hz * 2\n",
        "    overlap = NFFT - int(shift * fs_Hz)\n",
        "    psd, freqs = mlab.psd(np.squeeze(ch),\n",
        "                          NFFT=NFFT,\n",
        "                          window=mlab.window_hanning,\n",
        "                          Fs=fs_Hz,\n",
        "                          noverlap=overlap\n",
        "                          )  # returns PSD power per Hz\n",
        "    # convert the units of the spectral data\n",
        "    spec_PSDperBin = spec_PSDperHz * fs_Hz / float(NFFT)\n",
        "    return psd, freqs  # dB re: 1 uV\n",
        "\n",
        "\n",
        "def get_spectral_content(ch, fs_Hz, shift=0.1):\n",
        "    NFFT = fs_Hz * 2\n",
        "    overlap = NFFT - int(shift * fs_Hz)\n",
        "    spec_PSDperHz, spec_freqs, spec_t = mlab.specgram(np.squeeze(ch),\n",
        "                                                      NFFT=NFFT,\n",
        "                                                      window=mlab.window_hanning,\n",
        "                                                      Fs=fs_Hz,\n",
        "                                                      noverlap=overlap\n",
        "                                                      )  # returns PSD power per Hz\n",
        "    # convert the units of the spectral data\n",
        "    spec_PSDperBin = spec_PSDperHz * fs_Hz / float(NFFT)\n",
        "    return spec_t, spec_freqs, spec_PSDperBin  # dB re: 1 uV\n",
        "\n",
        "\n",
        "def plot_specgram(spec_freqs, spec_PSDperBin, title, shift, i=1):\n",
        "    f_lim_Hz = [0, 20]   # frequency limits for plotting\n",
        "    # plt.figure(figsize=(10,5))\n",
        "    spec_t = [idx * .1 for idx in range(len(spec_PSDperBin[0]))]\n",
        "    plt.subplot(3, 1, i)\n",
        "    plt.title(title)\n",
        "    plt.pcolor(spec_t, spec_freqs, 10 *\n",
        "               np.log10(spec_PSDperBin))  # dB re: 1 uV\n",
        "    plt.clim([-25, 26])\n",
        "    plt.xlim(spec_t[0], spec_t[-1] + 1)\n",
        "    plt.ylim(f_lim_Hz)\n",
        "    plt.xlabel('Time (sec)')\n",
        "    plt.ylabel('Frequency (Hz)')\n",
        "    plt.subplots_adjust(hspace=1)\n",
        "\n",
        "\n",
        "def resize_min(specgram, i=1):\n",
        "    min_length = min([len(el[0]) for el in specgram])\n",
        "    specgram = np.array([el[:, :min_length] for el in specgram])\n",
        "    return specgram\n",
        "\n",
        "\n",
        "def resize_max(specgram, fillval=np.nan):\n",
        "    max_length = max([len(el[0]) for el in specgram])\n",
        "    return np.array([pad_block(el, max_length, fillval) for el in specgram])\n",
        "\n",
        "\n",
        "def pad_block(block, max_length, fillval):\n",
        "    padding = np.full([len(block), max_length - (len(block[0]))], fillval)\n",
        "    return np.hstack((block, padding))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKntjk9QIgFU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ELECTRODE_C3 = 4 # Nuestro electrodo 5 \n",
        "ELECTRODE_C1 = 1\n",
        "ELECTRODE_C2 = -2\n",
        "ELECTRODE_C4 = 7 # # Nuestro electrodo 8\n",
        "\n",
        "SAMPLING_FREQ = 256\n",
        "DATA_COLUMNS = (1, 2, 3, 4, 5, 6, 7, 8, 13)\n",
        "LABELS = ['Right', 'Left', 'Rest']\n",
        "\n",
        "MARKER_DATA = {\"1_011_Rest20LeftRight20_MI-2019-3-24-16-25-41.csv\": '011_1to3_OpenBCI-RAW-2019-03-24_16-21-59.txt',\n",
        "               \"2_011_Rest20LeftRight20_MI-2019-3-24-16-38-10.csv\": '011_1to3_OpenBCI-RAW-2019-03-24_16-21-59.txt',\n",
        "               \"3_011_Rest20LeftRight10_MI-2019-3-24-16-49-23.csv\": '011_1to3_OpenBCI-RAW-2019-03-24_16-21-59.txt',\n",
        "               \"4_011_Rest20LeftRight10_MI-2019-3-24-16-57-8.csv\": '011_4to6_OpenBCI-RAW-2019-03-24_16-54-15.txt',\n",
        "               \"5_011_Rest20LeftRight20_MI-2019-3-24-17-3-17.csv\": '011_4to6_OpenBCI-RAW-2019-03-24_16-54-15.txt',\n",
        "          \n",
        "               }\n",
        "          \n",
        "FILES_BY_SUBJECT = [         \n",
        "             [\"data/March24_011/1_011_Rest20LeftRight20_MI-2019-3-24-16-25-41.csv\",  # 011\n",
        "              \"data/March24_011/2_011_Rest20LeftRight20_MI-2019-3-24-16-38-10.csv\",\n",
        "              \"data/March24_011/3_011_Rest20LeftRight10_MI-2019-3-24-16-49-23.csv\",\n",
        "              \"data/March24_011/4_011_Rest20LeftRight10_MI-2019-3-24-16-57-8.csv\",\n",
        "              \"data/March24_011/5_011_Rest20LeftRight20_MI-2019-3-24-17-3-17.csv\",\n",
        "              ]           \n",
        "             ]\n",
        "ALL_FILES = [name for sublist in FILES_BY_SUBJECT for name in sublist]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zgo_QzT3TTIh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6p9XdDgd3GUP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Preparamos nuestro dataset\n",
        "import os\n",
        "\n",
        "dir = '/content/drive/My Drive/Datasets/dataframes/16_channels'\n",
        "files = sorted([dir+'/'+f for f in os.listdir(dir)])\n",
        "\n",
        "eeg_data0 = np.zeros((2560,16))\n",
        "eeg_data1 = np.zeros((2560,16))\n",
        "eeg_data2 = np.zeros((2560,16))\n",
        "\n",
        "\n",
        "\n",
        "right_list = []\n",
        "left_list = []\n",
        "rest_list = []\n",
        "\n",
        "for f in files:\n",
        "  array_f = np.load(f)\n",
        "  array_f_0 = array_f[0:2560,:]\n",
        "  array_f_1 = array_f[2560:5120,:]\n",
        "  array_f_2 = array_f[5120:,:]\n",
        "\n",
        "  array_f_0 = array_f_0[:,4:12]\n",
        "  array_f_1 = array_f_1[:,4:12]\n",
        "  array_f_2 = array_f_2[:,4:12]\n",
        "\n",
        "  eeg_data0 = array_f_0\n",
        "  eeg_data1 = array_f_1\n",
        "  eeg_data2 = array_f_2\n",
        "\n",
        "  assert len(eeg_data0) == len(eeg_data1) == len(eeg_data2)\n",
        "\n",
        "  right_list.append(eeg_data0)\n",
        "  left_list.append(eeg_data1)\n",
        "  rest_list.append(eeg_data2)\n",
        "  \n",
        "train_data = {'Left':left_list,'Right':right_list,'Rest': rest_list}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKzRwccTTukw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e08c6438-3c3d-4acb-eee2-126b8ba3781f"
      },
      "source": [
        "len(train_data['Left'])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxSuKbT0H1Bw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "fae02f36-201c-4328-b36b-e2d87f822a89"
      },
      "source": [
        "import warnings\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn import model_selection\n",
        "from sklearn.preprocessing import scale\n",
        "from sklearn.decomposition import PCA\n",
        "import seaborn as sn\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.mlab as mlab\n",
        "import numpy.fft as fft\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "from scipy import signal\n",
        "\n",
        "import sys\n",
        "sys.path.append('../utils')\n",
        "#from metadata import MARKER_DATA, LABELS, FILES_BY_SUBJECT, ALL_FILES, ELECTRODE_C3, ELECTRODE_C4\n",
        "#import file_utils\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, classes,\n",
        "                          normalize=False,\n",
        "                          title=None,\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if not title:\n",
        "        if normalize:\n",
        "            title = 'Normalized confusion matrix'\n",
        "        else:\n",
        "            title = 'Confusion matrix, without normalization'\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    # Only use the labels that appear in the data\n",
        "    classes = classes[unique_labels(y_true, y_pred)]\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    # We want to show all ticks...\n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\n",
        "           yticks=np.arange(cm.shape[0]),\n",
        "           # ... and label them with the respective list entries\n",
        "           xticklabels=classes, yticklabels=classes,\n",
        "           title=title,\n",
        "           ylabel='True label',\n",
        "           xlabel='Predicted label')\n",
        "\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "             rotation_mode=\"anchor\")\n",
        "\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], fmt),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    fig.tight_layout()\n",
        "    return ax\n",
        "\n",
        "\n",
        "\n",
        "def plot_specgram(spec_freqs, spec_PSDperBin, title, shift, i=1):\n",
        "    f_lim_Hz = [0, 20]   # frequency limits for plotting\n",
        "    spec_t = [idx*.1 for idx in range(len(spec_PSDperBin[0]))]\n",
        "    plt.subplot(3, 1, i)\n",
        "    plt.title(title)\n",
        "    plt.pcolor(spec_t, spec_freqs, 10*np.log10(spec_PSDperBin))  # dB re: 1 uV\n",
        "    plt.clim([-25, 26])\n",
        "    plt.xlim(spec_t[0], spec_t[-1]+1)\n",
        "    plt.ylim(f_lim_Hz)\n",
        "    plt.xlabel('Time (sec)')\n",
        "    plt.ylabel('Frequency (Hz)')\n",
        "    plt.subplots_adjust(hspace=1)\n",
        "\n",
        "\n",
        "def epoch_data(data, window_length, shift, maxlen=2560):\n",
        "    arr = []\n",
        "    start = 0\n",
        "    i = start\n",
        "    maxlen = min(len(data), maxlen)\n",
        "    while i + window_length < start + maxlen:\n",
        "        arr.append(data[i:i+window_length])\n",
        "        i += shift\n",
        "    return np.array(arr)\n",
        "\n",
        "\n",
        "def extract_features(all_data, window_s, shift, plot_psd=False, separate_trials=False, scale_by=None):\n",
        "    all_psds = {label: [] for label in LABELS}\n",
        "    all_features = {label: [] for label in LABELS}\n",
        "\n",
        "    idx = 1\n",
        "    for direction, data in all_data.items():\n",
        "        for trial in data:\n",
        "            epochs = epoch_data(trial, int(256 * window_s), int(shift*256))    # shape n x 500 x 2\n",
        "            trial_features = []\n",
        "            for epoch in epochs:\n",
        "                print('epoch_shape: ',epoch.shape)\n",
        "                features, freqs, psds_per_channel = get_features(epoch.T, scale_by=scale_by)\n",
        " \n",
        "                psd_c3, psd_c4 = psds_per_channel[0] , psds_per_channel[-1]\n",
        "                all_psds[direction].append([psd_c3, psd_c4])\n",
        "                trial_features.append(features)\n",
        "                \n",
        "                # Sanity check: plot the psd\n",
        "                if plot_psd:\n",
        "                    plt.figure(\"psd\")\n",
        "                    plt.subplot(3, 2, idx)\n",
        "                    plt.plot(freqs, psd_c3)\n",
        "                    plt.ylim([0, 25])\n",
        "                    plt.xlim([6, 20])\n",
        "                    plt.subplot(3, 2, idx+1)\n",
        "                    plt.plot(freqs, psd_c4)\n",
        "                    plt.ylim([0, 25])\n",
        "                    plt.xlim([6, 20])\n",
        "            if trial_features:\n",
        "                if separate_trials:\n",
        "                    all_features[direction].append(np.array(trial_features))\n",
        "                else:\n",
        "                    all_features[direction].extend(trial_features)\n",
        "        idx += 2\n",
        "    return all_psds, all_features, freqs\n",
        "\n",
        "\n",
        "def to_feature_vec(all_features, rest=False):\n",
        "    feature_arr = []\n",
        "    for direction, features in all_features.items():\n",
        "        features = np.array(features)\n",
        "        arr = np.hstack((features, np.full([features.shape[0], 1], LABELS.index(direction))))\n",
        "        feature_arr.append(arr)\n",
        "    if not rest or not len(features):\n",
        "        feature_arr = feature_arr[:-1]\n",
        "    return np.vstack(feature_arr)\n",
        "\n",
        "\n",
        "def normalize(features_dict):\n",
        "    \"\"\" Divide features by mean per channel \"\"\"\n",
        "    all_features = to_feature_vec(features_dict, rest=True)\n",
        "    av = list(np.mean(all_features, axis=0))\n",
        "    mean_coeff = np.array([el/sum(av[:-1]) for el in av[:-1]])\n",
        "    for direction, features in features_dict.items():\n",
        "        features = [np.divide(example, mean_coeff) for example in features]\n",
        "        features_dict[direction] = features\n",
        "    \n",
        "def running_mean(x, N):\n",
        "   cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
        "   return (cumsum[N:] - cumsum[:-N]) / N\n",
        "\n",
        "\n",
        "def evaluate_models(X, Y, X_test, Y_test, models):\n",
        "    \"\"\" Evaluate test accuracy of all models in a list of models\n",
        "    \n",
        "    Args:\n",
        "        X : array of features (train)\n",
        "        Y : array of labels (train)\n",
        "        X_test : array of features (test)\n",
        "        Y_test : array of labels (test)\n",
        "        models : list of (name, model) tuples\n",
        "    \n",
        "    Returns:\n",
        "        val_results : array of accuracies, shape num_models\n",
        "    \"\"\"\n",
        "    test_results = []\n",
        "    for name, model in models:\n",
        "        model.fit(X, Y)\n",
        "        score = model.score(X_test, Y_test)\n",
        "        test_results.append(score)\n",
        "        msg = \"%s: %f\" % (name, score)\n",
        "        print(msg)\n",
        "    return np.array(test_results)\n",
        "\n",
        "\n",
        "def evaluate_models_crossval(X, Y, models, scoring, random_state, n_splits=10):\n",
        "    \"\"\" Evaluate cross-validation accuracy of all models in a list of models\n",
        "    \n",
        "    Args:\n",
        "        X : array of features\n",
        "        Y : array of labels\n",
        "        models : list of (name, model) tuples\n",
        "        scoring : string, scoring metric\n",
        "        random_state : seed to use for shuffling\n",
        "        n_splits : number of folds\n",
        "    \n",
        "    Returns:\n",
        "        val_results : array of accuracies, shape num_models x n_splits\n",
        "    \"\"\"\n",
        "    val_results = []\n",
        "    for name, model in models:\n",
        "        kfold = model_selection.KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
        "        print(kfold)\n",
        "        cv_results = model_selection.cross_val_score(model, X_test, Y_test, cv=kfold, scoring=scoring)\n",
        "        val_results.append(cv_results)\n",
        "        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "        print(msg)\n",
        "    return np.array(val_results)\n",
        "\n",
        "\n",
        "def get_features(arr, channels=[ELECTRODE_C3, ELECTRODE_C4], scale_by=None):\n",
        "    \"\"\" Get features from single window of EEG data\n",
        "    \n",
        "    Args:\n",
        "        arr : data of shape num_channels x timepoints\n",
        "    \n",
        "    Returns:\n",
        "        features : array with feature values\n",
        "        freqs : array of frequencies in Hz\n",
        "        psds_per_channel : array with full psd spectrum, shape num_channels x num_freqs\n",
        "    \"\"\"\n",
        "\n",
        "    psds_per_channel = []\n",
        "    nfft = 500\n",
        "    if arr.shape[-1] < 500:\n",
        "        nfft = 250\n",
        "      \n",
        "    for ch in arr[channels]:        \n",
        "        #freqs,psd = signal.periodogram(np.squeeze(ch),fs=250, nfft=500, detrend='constant')\n",
        "        psd, freqs, = mlab.psd(np.squeeze(ch),NFFT=nfft,Fs=256)\n",
        "        psds_per_channel.append(psd)\n",
        "    psds_per_channel = np.array(psds_per_channel)\n",
        "    mu_indices = np.where(np.logical_and(freqs >= 10, freqs <= 12))\n",
        "    \n",
        "    #features = np.amax(psds_per_channel[:,mu_indices], axis=-1).flatten()   # max of 10-12hz as feature\n",
        "    features = np.mean(psds_per_channel[:, mu_indices], axis=-1).flatten()   # mean of 10-12hz as feature\n",
        "    if scale_by:\n",
        "        scale_indices = np.where(np.logical_and(freqs >= scale_by[0], freqs <= scale_by[-1]))\n",
        "        scales = np.mean(psds_per_channel[:,scale_indices],axis=-1).flatten()\n",
        "        temp.append(scales)\n",
        "        features = np.divide(features, scales)\n",
        "    #features = np.array([features[:2].mean(), features[2:].mean()])\n",
        "    # features = psds_per_channel[:,mu_indices].flatten()                     # all of 10-12hz as feature\n",
        "    return features, freqs, psds_per_channel\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    shift = 0.0 #0.25\n",
        "    plot_psd = False\n",
        "    tmin, tmax = 0, 0\n",
        "    window_lengths = [1, 2, 4]  # window lengths in seconds\n",
        "    normalize_spectra = True\n",
        "    run_pca = False\n",
        "    scale_by = None\n",
        "    \n",
        "    # Load data\n",
        "    #dataset = load_all()\n",
        "    subjects = [0]             # index of the test files we want to use\n",
        "    \n",
        "    all_val_results = []\n",
        "    all_test_results = []\n",
        "\n",
        "    # Test options and evaluation metric\n",
        "    scoring = 'accuracy'\n",
        "    validation = True\n",
        "    test = False\n",
        "    seed = 7\n",
        "    models = []\n",
        "    models.append(('LR', LogisticRegression(solver='lbfgs')))\n",
        "    models.append(('LDA', LinearDiscriminantAnalysis()))\n",
        "    models.append(('KNN', KNeighborsClassifier()))\n",
        "    models.append(('CART', DecisionTreeClassifier()))\n",
        "    models.append(('NB', GaussianNB(var_smoothing=0.001)))\n",
        "    models.append(('SVM', SVC(gamma='scale')))\n",
        "    \n",
        "    # Perform leave-one-subject-out cross-validation for each subject\n",
        "    # Delivers accuracy by subject, then by window size, then by session, then by model\n",
        "    for subj in subjects:\n",
        "    \n",
        "        #test_csvs = FILES_BY_SUBJECT[subj]\n",
        "        #train_csvs = [el for el in ALL_FILES if el not in test_csvs]\n",
        "        #train_csvs = test_csvs.copy()\n",
        "        #train_data = merge_all_dols([dataset[csv] for csv in train_csvs]) \n",
        "        # Print subject name\n",
        "        #print(test_csvs[0].split('/')[1])\n",
        "        window_val_results = []\n",
        "        window_test_results = []\n",
        "    \n",
        "\n",
        "        \n",
        "    for window_s in window_lengths:\n",
        "        train_psds, train_features, freqs = extract_features(train_data, window_s, shift, plot_psd, scale_by=scale_by)\n",
        "        data = to_feature_vec(train_features, rest=False)\n",
        "        \n",
        "        # X, Y for training\n",
        "        # For testing: X_test, Y_test\n",
        "        X = data[:, :-1]\n",
        "        Y = data[:, -1]\n",
        "        X, Y = shuffle(X, Y, random_state=seed)\n",
        "\n",
        "        #X = X[:1100,:]\n",
        "        #X_test = X[1100:,:]\n",
        "\n",
        "        #Y = Y[:1100]\n",
        "        #Y_test = Y[1100:]\n",
        "        \n",
        "        if run_pca:\n",
        "            pca = PCA(n_components=2, svd_solver='full')\n",
        "            pca.fit(X)\n",
        "            X = pca.transform(X)\n",
        "        \n",
        "        subj_val_results = []\n",
        "        subj_test_results = []\n",
        "        #for csv in test_csvs:\n",
        "        # test_data = dataset[csv]\n",
        "\n",
        "        test_data = train_data\n",
        "\n",
        "        _, test_features, _ = extract_features(test_data, window_s, shift, plot_psd, scale_by=scale_by)\n",
        "        if normalize_spectra:\n",
        "            normalize(test_features)\n",
        "            \n",
        "        test_data = to_feature_vec(test_features)\n",
        "        X_test = test_data[:, :-1]\n",
        "        Y_test = test_data[:, -1]\n",
        "\n",
        "        if run_pca:\n",
        "            X_test = pca.transform(X_test)\n",
        "        \n",
        "        if validation:\n",
        "            print(\"VALIDATION\")\n",
        "            val_results = evaluate_models_crossval(X_test, Y_test, models, scoring, seed, n_splits=10)\n",
        "            print(\"average accuracy: \" + \"{:2.1f}\".format(val_results.mean() * 100))\n",
        "            subj_val_results.append(val_results.mean() * 100)\n",
        "           \n",
        "        if test:\n",
        "            print(\"TEST\")\n",
        "            test_results = evaluate_models(X, Y, X_test, Y_test, models)\n",
        "            print(\"average accuracy: {:2.1f}\".format(test_results.mean() * 100))\n",
        "            subj_test_results.append(test_results.mean() * 100)\n",
        "    window_val_results.append([np.array(subj_val_results).mean(), stats.sem(np.array(subj_val_results))])\n",
        "    window_test_results.append([np.array(subj_test_results).mean(),stats.sem(np.array(subj_test_results))])\n",
        "all_val_results.append(window_val_results)\n",
        "all_test_results.append(window_test_results)\n",
        "    \n",
        "print(np.array(all_test_results).mean())\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-7e6989f6cab7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mwindow_s\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwindow_lengths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0mtrain_psds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshift\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_psd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_by\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscale_by\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_feature_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-7e6989f6cab7>\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(all_data, window_s, shift, plot_psd, separate_trials, scale_by)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdirection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtrial\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mwindow_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshift\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# shape n x 500 x 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0mtrial_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-7e6989f6cab7>\u001b[0m in \u001b[0;36mepoch_data\u001b[0;34m(data, window_length, shift, maxlen)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mmaxlen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mwindow_length\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mwindow_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mshift\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjgZaP9X6lEX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}